{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bag of Words( BoW )\n",
    "\n",
    "- Bag of Words는 단어들의 순서는 전혀 고려하지 않고, 단어들의 출현 빈도( frequency )에만 집중하는 텍스트 데이터의 수치적 표현 방법( Local Representation )\n",
    "- 갖고 있는 어떤 텍스트 문서에 있는 단어들을 가방에 전부 넣은 다음 가방을 흔들어 단어들을 섞고, 해당 문서 내에서 특정 단어가 n번 등장했다면, 가방에서 해당 특정 단어 n개가 있게 된다. 여기서 가방을 흔들어서 단어를 섞었기 때문에 ***순서***는 중요하지 않다.\n",
    "\n",
    "- BoW 만드는 과정\n",
    "    1. 각 단어에 대한 고유한 정수 인덱스를 부여한다.\n",
    "    2. 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # 정규 표현식( regular expression )\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"정부가 발표하는 물가상승류과 소비자가 느끼는 물가상승률은 다르다\"를\n",
    "# BoW로 표현\n",
    "# 정규 표현식을 이용한 '.'제거하는 clearning 작업\n",
    "token = re.sub( \"(\\.)\", \"\", \n",
    "    \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['정부', '가', '발표', '하는', '물가상승률', '과', '소비자', '가', '느끼는', '물가상승률', '은', '다르다']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = okt.morphs( token )\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {}\n",
    "bow = []\n",
    "\n",
    "for vocab in token:\n",
    "    if vocab not in word2index.keys():\n",
    "        # 최초 등장 단어에 대한 단어 사전 등록 및 초기값 부여\n",
    "        word2index[ vocab ] = len( word2index )\n",
    "        bow.insert( len( word2index ) - 1, 1 )\n",
    "    else:\n",
    "        # 단어 사전에 등록된 단어에 대한 빈도수 증가\n",
    "        index = word2index.get( vocab )\n",
    "        bow[ index ] = bow[ index ] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'정부': 0,\n",
       " '가': 1,\n",
       " '발표': 2,\n",
       " '하는': 3,\n",
       " '물가상승률': 4,\n",
       " '과': 5,\n",
       " '소비자': 6,\n",
       " '느끼는': 7,\n",
       " '은': 8,\n",
       " '다르다': 9}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장에 대한 정수 인덱스\n",
    "word2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 1, 2, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BoW\n",
    "bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BoW는 각 단어가 등장한 횟수를 수치화하는 텍스트 표현 방법이다.\n",
    "- 주로 어떤 단어가 얼마나 등장했는지를 기준으로 문서가 어떤 성격의 문서인지를 판단하는 작업에 사용된다.\n",
    "- 즉, 분류 문제나 여러 문서 간의 유사도를 구하는 문제에 주로 쓰인다.\n",
    "    - '달리기', '체력', '근력'과 같은 단어가 자주 등장하면 해당 문서를 체육 관련 문서로 분류할 수 있을 것이며, '미분', '방정식', '부등식'과 같은 단어가 자주 등장한다면 수학 관련 문서로 분류할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CountVectorizer 클래스로 BoW 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [ 'you know I want your love. because I love you' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 1 2 1]]\n"
     ]
    }
   ],
   "source": [
    "print( vector.fit_transform( corpus ).toarray() ) # BoW, 각 단어의 빈도수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 4, 'know': 1, 'want': 3, 'your': 5, 'love': 2, 'because': 0}\n"
     ]
    }
   ],
   "source": [
    "print( vector.vocabulary_ ) # 각 단어의 인덱스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불용어를 제거한 BoW 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 사용자 정의 불용어 사용\n",
    "text = [ \"Family is not an important thing. It's everything.\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = CountVectorizer( stop_words = [ 'the', 'a', 'an', 'is', 'not' ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.fit_transform( text ).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'family': 1, 'important': 2, 'thing': 4, 'it': 3, 'everything': 0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) CountVectorizer 제공 불용어 사용\n",
    "vector = CountVectorizer( stop_words = \"english\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.fit_transform( text ).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'family': 0, 'important': 1, 'thing': 2}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK 제공 불용어 사용\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words( 'english' )\n",
    "vector = CountVectorizer( stop_words = stop_words )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.fit_transform( text ).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'family': 1, 'important': 2, 'thing': 3, 'everything': 0}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 문서 단어 행렬( Document-Term Matrix, DTM )\n",
    "\n",
    "- 문서 단어 행렬( Document-Term Matrix, DTM )은 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것이다.\n",
    "- 즉, 각 문서에 대한 BoW를 하나의 행렬로 만든 것으로 생각할 수 있으며, BoW와 다른 표현 방법이 아니라 BoW 표현을 다수의 문서에 대해서 행렬로 표현하고 부르는 용어이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DTM 예\n",
    "\n",
    "문서 1 : 먹고 싶은 사과  \n",
    "문서 2 : 먹고 싶은 바나나  \n",
    "문서 3 : 길고 노란 바나나 바나나   \n",
    "문서 4 : 저는 과일이 좋아요  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 문서 1 ~ 문서 4에 대한 DTM\n",
    "\n",
    "|-|과일이|길고|노란|먹고|바나나|사과|싶은|저는|좋아요|  \n",
    "|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|  \n",
    "|문서 1|0|0|0|1|0|1|1|0|0|  \n",
    "|문서 2|0|0|0|1|1|0|1|0|0|  \n",
    "|문서 3|0|1|1|0|2|0|0|0|0|  \n",
    "|문서 4|1|0|0|0|0|0|0|1|1|  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문서 단어 행렬( Document-Term Matrix, DTM ) 한계\n",
    "\n",
    "1) 희소 표현( Sparse representation )\n",
    "    - 원-홧 벡터는 단어 집합의 크기가 벡터의 차원이 되고 대부분의 값이 0이 된다는 특징이 있다.\n",
    "    - 이 특징은 공간적 낭비와 계산 리소스를 증가시킬 수 있다는 점에서 원-홧 벡터의 단점이 된다.\n",
    "    - DTM도 원-홧 벡터와 동일한 단점을 갖는다.\n",
    "    - DTM에서의 각 행은 문서 벡터를 의미하고, 문서 벡터의 차원은 원-홧 벡터와 마찬가지로 전체 단어 집합의 크기를 가진다.\n",
    "    - 전체 코퍼스가 방대한 데이터라면 문서 벡터의 차원은 수백만의 차원을 가질 수 있고, 또한 많은 문서 벡터가 대부분의 값이 0을 가질 수도 있다.\n",
    "    - 원-홧 벡터나 DTM과 같은 대부분의 값이 0인 표현을 희소 벡터( sparse vector ) 또는 희소 행렬( sparse matrix )라 한다.\n",
    "    - 희소 벡터는 많은 양의 저장 공간과 계산을 위한 리소스가 필요하다.\n",
    "    - 따라서 텍스트 전처리시에 단어 집합의 크기를 줄이는 일은 BoW / DTM 표현은 사용하는 모델에서는 매우 중요하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 단순 빈도 수 기반 접근\n",
    "    - 여러 문서에 등장하는 모든 단어에 대해서 빈도 표기를 하는 방법은 때로 한계를 가지고 있다.\n",
    "    - 예로 영어에 대해서 DTM을 만들었을 때, 불용어인 the는 어떤 문서이든 자주 등장할 수 밖에 없다. 그런데 유사한 문서인지 비교하고 싶은 문서 1, 문서 2, 문서 3에서 동일하게 the가 빈도수가 높다고 해서 이 문서들이 유사한 문서라고 판단할 수 없다.\n",
    "    - 각 문서에는 중요한 단어와 불필요한 단어들이 혼재되어 있다. 따라서 DTM에 불용어와 중요한 단어에 대한 구별 방법이 별도로 제공되지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF( 단어 빈도-역 문서 빈도, Term Frequency-Inverse Document Frequency )\n",
    "\n",
    "- TF-IDF( 단어 빈도-역 문서 빈도, Term Frequency-Inverse Document Frequency )는 단어의 빈도수와 역 문서 빈도( 문서의 빈도에 특정 식을 취한다.)를 사용하여 DTM내의 각 단어들마다 중요한 정도를 가중치로 주는 방법\n",
    "- TF-IDF 만드는 과정\n",
    "    1. DTM을 만든다.\n",
    "    2. TF-IDF 가중치를 부여한다.\n",
    "- TF-IDF는 주로 문서의 유사도를 구하는 작업, 검색 시스템에서 검색 결과의 중요도를 정하는 작업, 문서 내에서 특정 단어의 중요도를 구하는 작업등에 쓰일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF는 TF와 IDF를 곱한 값을 의미한다.\n",
    "- 문서를 d, 단어를 t, 문서의 총 개수를 n이라 표현할 때 TF, DF, IDF는 다음과 같이 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) tf( d, t ) : 특정 문서 d에서의 특정 단어 t의 등장 회수\n",
    "    - TF는 DTM에서 단어들이 가진 값들이다. \n",
    "    - DTM이 각 문서에서의 각 단어의 등장 빈도를 나타내는 값이기 때문이다.\n",
    "    \n",
    "2) df( t ) : 특정 단어 t가 등장한 문서의 수\n",
    "    - 여기서 특정 단어가 각 문서, 또는 문서들에서 몇 번 등장했는지는 관심가지지 않으며 오직 특정 단어 t가 등장한 문서의 수에만 관심을 가진다.\n",
    "    \n",
    "3) idf( d, t ) : df( t )에 반비례하는 수  \n",
    "    $idf(d, t) = log(\\frac{n}{1+df(t)})$\n",
    "    - IDF는 DF의 역수를 취한다. \n",
    "    - log를 사용하지 않았을 때, IDF를 DF의 역수( $\\frac{n}{df(t)}$ 식 )로 사용한다면 총 문서의 수 n이 커질 수록, IDF의 값은 기하급수적으로 커지게 된다. 그렇기 때문에 log를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF는 모든 문서에서 자주 등장하는 단어는 중요도가 낮다고 판단하며, 특정 문서에서만 자주 등장하는 단어는 중요도가 높다고 판단한다.\n",
    "- TF-IDF 값이 낮으면 중요도가 낮은 거이며, TF-IDF 값이 크면 중요도가 큰 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파이썬으로 TF-IDF 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF를 테스트할 문서라 가정\n",
    "docs = [\n",
    "    '먹고 싶은 사과',\n",
    "    '먹고 싶은 바나나',\n",
    "    '길고 노란 바나나 바나나',\n",
    "    '저는 과일이 좋아요'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['좋아요', '사과', '길고', '과일이', '먹고', '바나나', '노란', '저는', '싶은']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = list( set( w for doc in docs for w in doc.split() ) )\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['과일이', '길고', '노란', '먹고', '바나나', '사과', '싶은', '저는', '좋아요']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.sort()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF, DF, IDF 함수 정의\n",
    "N = len( docs )\n",
    "\n",
    "def tf( t, d ):\n",
    "    return d.count( t )\n",
    "\n",
    "def idf( t ):\n",
    "    df = 0\n",
    "    for doc in docs:\n",
    "        df += t in doc\n",
    "    \n",
    "    return log( N / ( df + 1 ) )\n",
    "\n",
    "def tfidf( t, d ):\n",
    "    return tf( t, d ) * idf( t )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   과일이  길고  노란  먹고  바나나  사과  싶은  저는  좋아요\n",
       "0    0   0   0   1    0   1   1   0    0\n",
       "1    0   0   0   1    1   0   1   0    0\n",
       "2    0   1   1   0    2   0   0   0    0\n",
       "3    1   0   0   0    0   0   0   1    1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF 계산\n",
    "result = []\n",
    "\n",
    "for i in range( N ):\n",
    "    result.append( [] )\n",
    "    d = docs[ i ]\n",
    "    for j in range( len( vocab ) ):\n",
    "        t = vocab[ j ]\n",
    "        result[ -1 ].append( tf( t, d ) )\n",
    "        \n",
    "tf_ = pd.DataFrame( result, columns = vocab )\n",
    "tf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>과일이</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>길고</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>노란</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>먹고</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>바나나</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>사과</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>싶은</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>저는</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>좋아요</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IDF\n",
       "과일이  0.693147\n",
       "길고   0.693147\n",
       "노란   0.693147\n",
       "먹고   0.287682\n",
       "바나나  0.287682\n",
       "사과   0.693147\n",
       "싶은   0.287682\n",
       "저는   0.693147\n",
       "좋아요  0.693147"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IDF( 역문서 빈도 ) 계산\n",
    "result = []\n",
    "\n",
    "for j in range( len( vocab ) ):\n",
    "    t = vocab[ j ]\n",
    "    result.append( idf( t ) )\n",
    "    \n",
    "idf_ = pd.DataFrame( result, index = vocab, columns = [ 'IDF' ] )\n",
    "idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>과일이</th>\n",
       "      <th>길고</th>\n",
       "      <th>노란</th>\n",
       "      <th>먹고</th>\n",
       "      <th>바나나</th>\n",
       "      <th>사과</th>\n",
       "      <th>싶은</th>\n",
       "      <th>저는</th>\n",
       "      <th>좋아요</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        과일이        길고        노란        먹고       바나나        사과        싶은  \\\n",
       "0  0.000000  0.000000  0.000000  0.287682  0.000000  0.693147  0.287682   \n",
       "1  0.000000  0.000000  0.000000  0.287682  0.287682  0.000000  0.287682   \n",
       "2  0.000000  0.693147  0.693147  0.000000  0.575364  0.000000  0.000000   \n",
       "3  0.693147  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         저는       좋아요  \n",
       "0  0.000000  0.000000  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000000  0.000000  \n",
       "3  0.693147  0.693147  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF 행렬 계산\n",
    "result = []\n",
    "\n",
    "for i in range( N ):\n",
    "    result.append( [] )\n",
    "    d = docs[ i ]\n",
    "    for j in range( len( vocab ) ):\n",
    "        t = vocab[ j ]\n",
    "        result[ -1 ].append( tfidf( t, d ) )\n",
    "\n",
    "tfidf_ = pd.DataFrame( result, columns = vocab )\n",
    "tfidf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사이킷런을 이용한 DTM과 TF-IDF 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'you know I want your love',\n",
    "    'I like you',\n",
    "    'what should I do'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 0 1 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 0]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "# DTM\n",
    "print( vector.fit_transform( corpus ).toarray() )\n",
    "print( vector.vocabulary_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfv = TfidfVectorizer().fit( corpus )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46735098 0.         0.46735098 0.         0.46735098\n",
      "  0.         0.35543247 0.46735098]\n",
      " [0.         0.         0.79596054 0.         0.         0.\n",
      "  0.         0.60534851 0.        ]\n",
      " [0.57735027 0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.        ]]\n",
      "{'you': 7, 'know': 1, 'want': 5, 'your': 8, 'love': 3, 'like': 2, 'what': 6, 'should': 4, 'do': 0}\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "print( tfidfv.transform( corpus ).toarray() )\n",
    "print( tfidfv.vocabulary_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 순환신경망( Recurrent Neural Network, RNN )\n",
    "\n",
    "- RNN( Recurrent Neural Network )은 시퀀스( Sequence ) 모델이다.\n",
    "- 입력과 출력을 시퀀스 단위로 처리하는 모델이다.\n",
    "    - 번역기를 예로 들면 입력은 번역하고자 하는 문장( 단어 시퀀스 )이고, 출력은 해당되는 번역된 문장 또는 단어 시퀀스이다.\n",
    "- 시퀀스들을 처리하기 위해 고안된 모델들을 시퀀스 모델이라 하고, RNN은 Deep Learning에서 가장 기본적인 시퀀스 모델이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 순환 신경망( Recurrent Neural Network, RNN )\n",
    "\n",
    "- 피드포워드 신경망( FFNN )은 전부 은닉층에서 활성화 함수를 지난 값은 오직 출력층 방향으로만 향하였다.\n",
    "- RNN은 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력층 방향으로도 보내면서, 다시 은닉층 노드의 다음 계산으로 보내는 특징을 갖고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text]( rnn_image1_ver2.png )\n",
    "\n",
    "- x는 입력층의 입력벡터, y는 출력층의 출력벡터, 편향 b도 입력으로 존재하지만 그림에서는 생략\n",
    "- RNN에서 은닉층에서 활성화 함수를 통해 결과를 내보내는 역활을 하는 노드를 셀( cell )이라 한다.\n",
    "- 셀은 이전의 값을 기억하려고 하는 일종의 메모리 역활을 수행하므로 이를 메모리 셀 또는 RNN 셀이라 표현한다.\n",
    "\n",
    "\n",
    "- 은닉층의 메모리 셀을 각각의 시점( time step )에서 바로 이전 시점에서의 은닉층의 메모리 셀에서 나온 값을 자신의 입력으로 사용하는 재귀적 활동을 한다.\n",
    "- 현재 시점을 변수 t로 표현한다.\n",
    "- 현재 시점 t에서의 메모리 셀이 갖고있는 값은 과거의 메모리 셀들의 값에 영향을 받은 것임을 의미한다.\n",
    "\n",
    "\n",
    "- 메모리 셀이 출력층 방향으로 또는 다음 시점 t + 1의 자신에게 보내는 값을 은닉 상태( hidden state )라고 한다. 즉, t 시점의 메모리 셀은 t - 1 시점의 메모리 셀이 보낸 은닉 상태값을 t 시점의 은닉 상태 계산을 위한 입력값으로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text]( rnn_image2_ver3.png )\n",
    "\n",
    "- 위 그림은 RNN을 표현할 때 일반적으로 좌측과 같이 화살표로 사이클을 그려서 재귀 형태로 표현하기도 하고, 사이클을 그리는 화살표 대신 여러 시점으로 펼쳐서 표현하기도 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 피드 포워드 신경망에서는 뉴런이라는 단위를 사용하지만, RNN에서는 뉴런이라는 단위보다는 입력층과 출력층에서는 각각 입력 벡터, 출력 벡터라 하고, 은닉층에서는 은닉 상태라는 표현을 주로 사용한다.\n",
    "\n",
    "![Alt text]( rnn_image2.5.png )\n",
    "\n",
    "- 이 그림은 RNN을 뉴런 단위로 표현한 내용으로 입력 벡터 4차원, 은닉 상태 크기 2, 출력 벡터 2차원인 RNN이 시점이 2일 때의 모습을 표현한 그림이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text]( rnn_image3_ver2.png )\n",
    "\n",
    "- RNN은 입력과 출력의 길이를 다르게 설계 할 수 있으므로 다양한 용도로 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하나의 입력에 대해서 여러개의 출력( one-to-many )의 모델은 하나의 이미지 입력에 대해서 사진의 제목을 출력하는 이미지 캡셔닝( Image Captioning ) 작업에 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text]( rnn_image3.5.png )\n",
    "\n",
    "- 단어 시퀀스에 대해서 하나의 출력( many-to-one )을 하는 모델은 입력 문서가 긍정적인지 부정적인지를 판별하는 감성 분류( sentiment classification ), 또는 메일이 정상 메일인지 스팸 메일인지 판별하는 스팸 메일 분류( spam detection )에 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text]( rnn_image3.7.png )\n",
    "\n",
    "- 다 대 다( many-to-many )의 모델의 경우에는 입력 문장으로 부터 대답 문자을 출력하는 챗봇과 입력 문장으로부터 번역된 문장을 출력하는 번역기, 개체명 인식이나 품사 태깅 작업을 수행하는 경우에 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN 수식\n",
    "\n",
    "![Alt text]( rnn_image7_ver2.png )\n",
    "\n",
    "- 현재 시점 t에서의 은닉 상태값을 $h_{t}$라고 정의하고, 은닉층의 메모리 셀은 $h_{t}$를 계산하기 위해서 총 두 개의 가중치를 갖게 된다.\n",
    "- 하나는 입력층에서 입력값을 위한 가중치 $W_{x}$이고, 하나는 이전 시점   t-1의 은닉 상태값인 $h_{t-1}$을 위한 가중치 $W_{h}$이다.\n",
    "\n",
    "\n",
    "은닉층 : $h_{t} = tanh(W_{x} x_{t} + W_{h}h_{t−1} + b)$    \n",
    "출력층 : $y_{t} = f(W_{y}h_{t} + b)$  \n",
    "단, $f$는 비선형 활성화 함수중 하나이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RNN은 은닉층 연산을 벡터와 행렬 연산으로 이해할 수 있다.\n",
    "- 자연어 처리에서 RNN의 입력 $x_{t}$는 대부분의 경우에 단어 벡터로 간주할 수 있는데, 단어 벡터의 차원을 $d$라고 하고, 은닉 상태의 크기를 $D_{h}$라고 하였을 때 각 벡터와 행렬의 크기는 다음과 같다.\n",
    "\n",
    "$x_{t}$ : $(d x 1)$  \n",
    "$W_{x}$ : $(D_{h} x d)$  \n",
    "$W_{h}$ : $(D_{h} x D_{h})$  \n",
    "$h_{t-1}$ : $(D_{h} x 1 )$  \n",
    "$b$ : $(D_{h} x 1 )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배치 크기가 1이고, $d$와 $D_{h}$ 두 값 모두를 4로 가정하였을 때, RNN의 은닉층 연산은 다음과 같다.\n",
    "\n",
    "![Alt text]( rnn_images4-5.png )\n",
    "\n",
    "- $h_{t}$를 계산하기 위한 활성화 함수로는 주로 하이퍼볼릭탄젠트 함수(tanh)가 사용되지만, ReLU로 바꾸 사용하는 시도도 있다.\n",
    "- 각각의 가중치 $W_{x}, W_{h}, W_{y}$의 값은 모두 시점에서 값을 동일하게 공유한다. 만약 은닉층이 2개 이상일 경우에는 은닉층 2개의 가중치는 서로 다르다.\n",
    "- 출력층은 결과값인 $y_{t}$를 계산하기 위한 활성화 함수로는 상황에 따라 다르지만 예로 이진 분류를 해야하는 경우라면 시그모이드 함수를 사용할 수 있고 다양한 카테고리 중에서 선택해야하는 문제라면 소프트맥스 함수를 사용하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BPTT( Backpropagation through time )\n",
    "\n",
    "- RNN도 다른 인공 신경망과 마찬가지로 역전파를 통해 학습을 진행한다.\n",
    "- FFNN의 역전파와 다른 점이 있다면 RNN은 전체 시점에 대해서 네트워크를 펼친 다음에 역전파를 사용하며 모든 시점에 대해서 가중치를 공유하고 있다는 점이다.\n",
    "- RNN의 이러한 역전파 과정을 BPTT( Backpropagation through time, 시간에 따른 역전파 )라고 부른다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN을 이용한 텍스트 생성( Text Generation Using RNN )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"경마장에 있는 말이 뛰고 있다\\n\n",
    "그의 말이 법이다\\n\n",
    "가는 말이 고와야 오는 말이 곱다\\n\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리 및 EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합의 크기 : 12\n"
     ]
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts( [ text ] )\n",
    "vocab_size = len( t.word_index ) + 1\n",
    "print( '단어 집합의 크기 : {}'.format( vocab_size ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'말이': 1, '경마장에': 2, '있는': 3, '뛰고': 4, '있다': 5, '그의': 6, '법이다': 7, '가는': 8, '고와야': 9, '오는': 10, '곱다': 11}\n"
     ]
    }
   ],
   "source": [
    "# 정수 인덱스\n",
    "print( t.word_index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습에 사용할 샘플의 개수 : 11\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터 생성\n",
    "sequences = list()\n",
    "\n",
    "for line in text.split( '\\n' ):\n",
    "    encoded = t.texts_to_sequences( [ line ] )[ 0 ]\n",
    "    for i in range( 1, len( encoded ) ):\n",
    "        sequence = encoded[ :i + 1 ]\n",
    "        sequences.append( sequence )\n",
    "        \n",
    "print( '학습에 사용할 샘플의 개수 : {}'.format( len( sequences ) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3], [2, 3, 1], [2, 3, 1, 4], [2, 3, 1, 4, 5], [6, 1], [6, 1, 7], [8, 1], [8, 1, 9], [8, 1, 9, 10], [8, 1, 9, 10, 1], [8, 1, 9, 10, 1, 11]]\n"
     ]
    }
   ],
   "source": [
    "print( sequences )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "샘플의 최대 길이 : 6\n"
     ]
    }
   ],
   "source": [
    "max_len = max( len( l ) for l in sequences )\n",
    "print( '샘플의 최대 길이 : {}'.format( max_len ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pad_sequences( sequences, maxlen = max_len, padding = 'pre' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  2  3]\n",
      " [ 0  0  0  2  3  1]\n",
      " [ 0  0  2  3  1  4]\n",
      " [ 0  2  3  1  4  5]\n",
      " [ 0  0  0  0  6  1]\n",
      " [ 0  0  0  6  1  7]\n",
      " [ 0  0  0  0  8  1]\n",
      " [ 0  0  0  8  1  9]\n",
      " [ 0  0  8  1  9 10]\n",
      " [ 0  8  1  9 10  1]\n",
      " [ 8  1  9 10  1 11]]\n"
     ]
    }
   ],
   "source": [
    "print( sequences )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터와 레이블 분리\n",
    "sequences = np.array( sequences )\n",
    "X = sequences[ :, :-1 ]\n",
    "y = sequences[ :, -1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0  0  2]\n",
      " [ 0  0  0  2  3]\n",
      " [ 0  0  2  3  1]\n",
      " [ 0  2  3  1  4]\n",
      " [ 0  0  0  0  6]\n",
      " [ 0  0  0  6  1]\n",
      " [ 0  0  0  0  8]\n",
      " [ 0  0  0  8  1]\n",
      " [ 0  0  8  1  9]\n",
      " [ 0  8  1  9 10]\n",
      " [ 8  1  9 10  1]]\n"
     ]
    }
   ],
   "source": [
    "print( X ) # 입력 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  1  4  5  1  7  1  9 10  1 11]\n"
     ]
    }
   ],
   "source": [
    "print( y ) # 레이블"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical( y, num_classes = vocab_size )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print( y )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add( Embedding( vocab_size, 10, input_length = max_len - 1 ) )\n",
    "model.add( SimpleRNN( 32 ) )\n",
    "model.add( Dense( vocab_size, activation = 'softmax' ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11 samples\n",
      "Epoch 1/200\n",
      "11/11 - 1s - loss: 2.3340 - accuracy: 0.3636\n",
      "Epoch 2/200\n",
      "11/11 - 0s - loss: 2.3156 - accuracy: 0.3636\n",
      "Epoch 3/200\n",
      "11/11 - 0s - loss: 2.2965 - accuracy: 0.3636\n",
      "Epoch 4/200\n",
      "11/11 - 0s - loss: 2.2766 - accuracy: 0.3636\n",
      "Epoch 5/200\n",
      "11/11 - 0s - loss: 2.2560 - accuracy: 0.3636\n",
      "Epoch 6/200\n",
      "11/11 - 0s - loss: 2.2348 - accuracy: 0.3636\n",
      "Epoch 7/200\n",
      "11/11 - 0s - loss: 2.2129 - accuracy: 0.3636\n",
      "Epoch 8/200\n",
      "11/11 - 0s - loss: 2.1904 - accuracy: 0.3636\n",
      "Epoch 9/200\n",
      "11/11 - 0s - loss: 2.1674 - accuracy: 0.3636\n",
      "Epoch 10/200\n",
      "11/11 - 0s - loss: 2.1441 - accuracy: 0.3636\n",
      "Epoch 11/200\n",
      "11/11 - 0s - loss: 2.1205 - accuracy: 0.3636\n",
      "Epoch 12/200\n",
      "11/11 - 0s - loss: 2.0968 - accuracy: 0.3636\n",
      "Epoch 13/200\n",
      "11/11 - 0s - loss: 2.0733 - accuracy: 0.3636\n",
      "Epoch 14/200\n",
      "11/11 - 0s - loss: 2.0501 - accuracy: 0.3636\n",
      "Epoch 15/200\n",
      "11/11 - 0s - loss: 2.0275 - accuracy: 0.3636\n",
      "Epoch 16/200\n",
      "11/11 - 0s - loss: 2.0057 - accuracy: 0.3636\n",
      "Epoch 17/200\n",
      "11/11 - 0s - loss: 1.9849 - accuracy: 0.3636\n",
      "Epoch 18/200\n",
      "11/11 - 0s - loss: 1.9654 - accuracy: 0.3636\n",
      "Epoch 19/200\n",
      "11/11 - 0s - loss: 1.9474 - accuracy: 0.3636\n",
      "Epoch 20/200\n",
      "11/11 - 0s - loss: 1.9309 - accuracy: 0.3636\n",
      "Epoch 21/200\n",
      "11/11 - 0s - loss: 1.9160 - accuracy: 0.3636\n",
      "Epoch 22/200\n",
      "11/11 - 0s - loss: 1.9027 - accuracy: 0.3636\n",
      "Epoch 23/200\n",
      "11/11 - 0s - loss: 1.8907 - accuracy: 0.3636\n",
      "Epoch 24/200\n",
      "11/11 - 0s - loss: 1.8798 - accuracy: 0.3636\n",
      "Epoch 25/200\n",
      "11/11 - 0s - loss: 1.8695 - accuracy: 0.3636\n",
      "Epoch 26/200\n",
      "11/11 - 0s - loss: 1.8596 - accuracy: 0.3636\n",
      "Epoch 27/200\n",
      "11/11 - 0s - loss: 1.8497 - accuracy: 0.3636\n",
      "Epoch 28/200\n",
      "11/11 - 0s - loss: 1.8394 - accuracy: 0.3636\n",
      "Epoch 29/200\n",
      "11/11 - 0s - loss: 1.8287 - accuracy: 0.3636\n",
      "Epoch 30/200\n",
      "11/11 - 0s - loss: 1.8173 - accuracy: 0.3636\n",
      "Epoch 31/200\n",
      "11/11 - 0s - loss: 1.8052 - accuracy: 0.3636\n",
      "Epoch 32/200\n",
      "11/11 - 0s - loss: 1.7925 - accuracy: 0.3636\n",
      "Epoch 33/200\n",
      "11/11 - 0s - loss: 1.7793 - accuracy: 0.3636\n",
      "Epoch 34/200\n",
      "11/11 - 0s - loss: 1.7656 - accuracy: 0.3636\n",
      "Epoch 35/200\n",
      "11/11 - 0s - loss: 1.7515 - accuracy: 0.3636\n",
      "Epoch 36/200\n",
      "11/11 - 0s - loss: 1.7371 - accuracy: 0.3636\n",
      "Epoch 37/200\n",
      "11/11 - 0s - loss: 1.7225 - accuracy: 0.3636\n",
      "Epoch 38/200\n",
      "11/11 - 0s - loss: 1.7077 - accuracy: 0.3636\n",
      "Epoch 39/200\n",
      "11/11 - 0s - loss: 1.6927 - accuracy: 0.3636\n",
      "Epoch 40/200\n",
      "11/11 - 0s - loss: 1.6773 - accuracy: 0.3636\n",
      "Epoch 41/200\n",
      "11/11 - 0s - loss: 1.6617 - accuracy: 0.3636\n",
      "Epoch 42/200\n",
      "11/11 - 0s - loss: 1.6457 - accuracy: 0.3636\n",
      "Epoch 43/200\n",
      "11/11 - 0s - loss: 1.6292 - accuracy: 0.3636\n",
      "Epoch 44/200\n",
      "11/11 - 0s - loss: 1.6122 - accuracy: 0.3636\n",
      "Epoch 45/200\n",
      "11/11 - 0s - loss: 1.5947 - accuracy: 0.4545\n",
      "Epoch 46/200\n",
      "11/11 - 0s - loss: 1.5766 - accuracy: 0.4545\n",
      "Epoch 47/200\n",
      "11/11 - 0s - loss: 1.5579 - accuracy: 0.4545\n",
      "Epoch 48/200\n",
      "11/11 - 0s - loss: 1.5387 - accuracy: 0.5455\n",
      "Epoch 49/200\n",
      "11/11 - 0s - loss: 1.5190 - accuracy: 0.5455\n",
      "Epoch 50/200\n",
      "11/11 - 0s - loss: 1.4988 - accuracy: 0.5455\n",
      "Epoch 51/200\n",
      "11/11 - 0s - loss: 1.4782 - accuracy: 0.5455\n",
      "Epoch 52/200\n",
      "11/11 - 0s - loss: 1.4572 - accuracy: 0.6364\n",
      "Epoch 53/200\n",
      "11/11 - 0s - loss: 1.4359 - accuracy: 0.6364\n",
      "Epoch 54/200\n",
      "11/11 - 0s - loss: 1.4144 - accuracy: 0.6364\n",
      "Epoch 55/200\n",
      "11/11 - 0s - loss: 1.3926 - accuracy: 0.6364\n",
      "Epoch 56/200\n",
      "11/11 - 0s - loss: 1.3707 - accuracy: 0.6364\n",
      "Epoch 57/200\n",
      "11/11 - 0s - loss: 1.3487 - accuracy: 0.6364\n",
      "Epoch 58/200\n",
      "11/11 - 0s - loss: 1.3266 - accuracy: 0.6364\n",
      "Epoch 59/200\n",
      "11/11 - 0s - loss: 1.3046 - accuracy: 0.6364\n",
      "Epoch 60/200\n",
      "11/11 - 0s - loss: 1.2826 - accuracy: 0.6364\n",
      "Epoch 61/200\n",
      "11/11 - 0s - loss: 1.2608 - accuracy: 0.6364\n",
      "Epoch 62/200\n",
      "11/11 - 0s - loss: 1.2393 - accuracy: 0.6364\n",
      "Epoch 63/200\n",
      "11/11 - 0s - loss: 1.2180 - accuracy: 0.6364\n",
      "Epoch 64/200\n",
      "11/11 - 0s - loss: 1.1970 - accuracy: 0.6364\n",
      "Epoch 65/200\n",
      "11/11 - 0s - loss: 1.1764 - accuracy: 0.6364\n",
      "Epoch 66/200\n",
      "11/11 - 0s - loss: 1.1562 - accuracy: 0.6364\n",
      "Epoch 67/200\n",
      "11/11 - 0s - loss: 1.1363 - accuracy: 0.6364\n",
      "Epoch 68/200\n",
      "11/11 - 0s - loss: 1.1170 - accuracy: 0.6364\n",
      "Epoch 69/200\n",
      "11/11 - 0s - loss: 1.0980 - accuracy: 0.6364\n",
      "Epoch 70/200\n",
      "11/11 - 0s - loss: 1.0794 - accuracy: 0.6364\n",
      "Epoch 71/200\n",
      "11/11 - 0s - loss: 1.0613 - accuracy: 0.6364\n",
      "Epoch 72/200\n",
      "11/11 - 0s - loss: 1.0435 - accuracy: 0.6364\n",
      "Epoch 73/200\n",
      "11/11 - 0s - loss: 1.0261 - accuracy: 0.6364\n",
      "Epoch 74/200\n",
      "11/11 - 0s - loss: 1.0090 - accuracy: 0.6364\n",
      "Epoch 75/200\n",
      "11/11 - 0s - loss: 0.9923 - accuracy: 0.6364\n",
      "Epoch 76/200\n",
      "11/11 - 0s - loss: 0.9759 - accuracy: 0.6364\n",
      "Epoch 77/200\n",
      "11/11 - 0s - loss: 0.9598 - accuracy: 0.6364\n",
      "Epoch 78/200\n",
      "11/11 - 0s - loss: 0.9440 - accuracy: 0.6364\n",
      "Epoch 79/200\n",
      "11/11 - 0s - loss: 0.9285 - accuracy: 0.6364\n",
      "Epoch 80/200\n",
      "11/11 - 0s - loss: 0.9132 - accuracy: 0.6364\n",
      "Epoch 81/200\n",
      "11/11 - 0s - loss: 0.8982 - accuracy: 0.6364\n",
      "Epoch 82/200\n",
      "11/11 - 0s - loss: 0.8835 - accuracy: 0.6364\n",
      "Epoch 83/200\n",
      "11/11 - 0s - loss: 0.8690 - accuracy: 0.6364\n",
      "Epoch 84/200\n",
      "11/11 - 0s - loss: 0.8547 - accuracy: 0.6364\n",
      "Epoch 85/200\n",
      "11/11 - 0s - loss: 0.8407 - accuracy: 0.7273\n",
      "Epoch 86/200\n",
      "11/11 - 0s - loss: 0.8268 - accuracy: 0.7273\n",
      "Epoch 87/200\n",
      "11/11 - 0s - loss: 0.8132 - accuracy: 0.7273\n",
      "Epoch 88/200\n",
      "11/11 - 0s - loss: 0.7997 - accuracy: 0.7273\n",
      "Epoch 89/200\n",
      "11/11 - 0s - loss: 0.7864 - accuracy: 0.7273\n",
      "Epoch 90/200\n",
      "11/11 - 0s - loss: 0.7733 - accuracy: 0.7273\n",
      "Epoch 91/200\n",
      "11/11 - 0s - loss: 0.7604 - accuracy: 0.7273\n",
      "Epoch 92/200\n",
      "11/11 - 0s - loss: 0.7476 - accuracy: 0.7273\n",
      "Epoch 93/200\n",
      "11/11 - 0s - loss: 0.7350 - accuracy: 0.7273\n",
      "Epoch 94/200\n",
      "11/11 - 0s - loss: 0.7225 - accuracy: 0.7273\n",
      "Epoch 95/200\n",
      "11/11 - 0s - loss: 0.7102 - accuracy: 0.7273\n",
      "Epoch 96/200\n",
      "11/11 - 0s - loss: 0.6980 - accuracy: 0.7273\n",
      "Epoch 97/200\n",
      "11/11 - 0s - loss: 0.6859 - accuracy: 0.7273\n",
      "Epoch 98/200\n",
      "11/11 - 0s - loss: 0.6740 - accuracy: 0.7273\n",
      "Epoch 99/200\n",
      "11/11 - 0s - loss: 0.6622 - accuracy: 0.7273\n",
      "Epoch 100/200\n",
      "11/11 - 0s - loss: 0.6506 - accuracy: 0.7273\n",
      "Epoch 101/200\n",
      "11/11 - 0s - loss: 0.6391 - accuracy: 0.7273\n",
      "Epoch 102/200\n",
      "11/11 - 0s - loss: 0.6278 - accuracy: 0.7273\n",
      "Epoch 103/200\n",
      "11/11 - 0s - loss: 0.6166 - accuracy: 0.7273\n",
      "Epoch 104/200\n",
      "11/11 - 0s - loss: 0.6055 - accuracy: 0.7273\n",
      "Epoch 105/200\n",
      "11/11 - 0s - loss: 0.5945 - accuracy: 0.7273\n",
      "Epoch 106/200\n",
      "11/11 - 0s - loss: 0.5837 - accuracy: 0.7273\n",
      "Epoch 107/200\n",
      "11/11 - 0s - loss: 0.5731 - accuracy: 0.8182\n",
      "Epoch 108/200\n",
      "11/11 - 0s - loss: 0.5626 - accuracy: 0.8182\n",
      "Epoch 109/200\n",
      "11/11 - 0s - loss: 0.5522 - accuracy: 0.8182\n",
      "Epoch 110/200\n",
      "11/11 - 0s - loss: 0.5419 - accuracy: 0.9091\n",
      "Epoch 111/200\n",
      "11/11 - 0s - loss: 0.5318 - accuracy: 0.9091\n",
      "Epoch 112/200\n",
      "11/11 - 0s - loss: 0.5219 - accuracy: 0.9091\n",
      "Epoch 113/200\n",
      "11/11 - 0s - loss: 0.5120 - accuracy: 0.9091\n",
      "Epoch 114/200\n",
      "11/11 - 0s - loss: 0.5023 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "11/11 - 0s - loss: 0.4927 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "11/11 - 0s - loss: 0.4833 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "11/11 - 0s - loss: 0.4740 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "11/11 - 0s - loss: 0.4648 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "11/11 - 0s - loss: 0.4558 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "11/11 - 0s - loss: 0.4469 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "11/11 - 0s - loss: 0.4382 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "11/11 - 0s - loss: 0.4295 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "11/11 - 0s - loss: 0.4210 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "11/11 - 0s - loss: 0.4127 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "11/11 - 0s - loss: 0.4045 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "11/11 - 0s - loss: 0.3964 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "11/11 - 0s - loss: 0.3884 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "11/11 - 0s - loss: 0.3806 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "11/11 - 0s - loss: 0.3729 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "11/11 - 0s - loss: 0.3653 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "11/11 - 0s - loss: 0.3579 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "11/11 - 0s - loss: 0.3506 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "11/11 - 0s - loss: 0.3434 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "11/11 - 0s - loss: 0.3364 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "11/11 - 0s - loss: 0.3295 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "11/11 - 0s - loss: 0.3227 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "11/11 - 0s - loss: 0.3161 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "11/11 - 0s - loss: 0.3096 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "11/11 - 0s - loss: 0.3032 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "11/11 - 0s - loss: 0.2969 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "11/11 - 0s - loss: 0.2908 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/200\n",
      "11/11 - 0s - loss: 0.2848 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "11/11 - 0s - loss: 0.2789 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "11/11 - 0s - loss: 0.2731 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "11/11 - 0s - loss: 0.2675 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "11/11 - 0s - loss: 0.2620 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "11/11 - 0s - loss: 0.2566 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "11/11 - 0s - loss: 0.2513 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "11/11 - 0s - loss: 0.2461 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "11/11 - 0s - loss: 0.2411 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "11/11 - 0s - loss: 0.2361 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "11/11 - 0s - loss: 0.2313 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "11/11 - 0s - loss: 0.2266 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "11/11 - 0s - loss: 0.2220 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "11/11 - 0s - loss: 0.2175 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "11/11 - 0s - loss: 0.2130 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "11/11 - 0s - loss: 0.2087 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "11/11 - 0s - loss: 0.2045 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "11/11 - 0s - loss: 0.2004 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "11/11 - 0s - loss: 0.1964 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "11/11 - 0s - loss: 0.1925 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "11/11 - 0s - loss: 0.1887 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "11/11 - 0s - loss: 0.1849 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "11/11 - 0s - loss: 0.1813 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "11/11 - 0s - loss: 0.1777 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "11/11 - 0s - loss: 0.1742 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "11/11 - 0s - loss: 0.1708 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "11/11 - 0s - loss: 0.1675 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "11/11 - 0s - loss: 0.1643 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "11/11 - 0s - loss: 0.1611 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "11/11 - 0s - loss: 0.1580 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "11/11 - 0s - loss: 0.1550 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "11/11 - 0s - loss: 0.1521 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "11/11 - 0s - loss: 0.1492 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "11/11 - 0s - loss: 0.1464 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "11/11 - 0s - loss: 0.1437 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "11/11 - 0s - loss: 0.1410 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "11/11 - 0s - loss: 0.1384 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "11/11 - 0s - loss: 0.1359 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "11/11 - 0s - loss: 0.1334 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "11/11 - 0s - loss: 0.1309 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "11/11 - 0s - loss: 0.1286 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "11/11 - 0s - loss: 0.1262 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "11/11 - 0s - loss: 0.1240 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "11/11 - 0s - loss: 0.1218 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "11/11 - 0s - loss: 0.1196 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "11/11 - 0s - loss: 0.1175 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "11/11 - 0s - loss: 0.1154 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "11/11 - 0s - loss: 0.1134 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "11/11 - 0s - loss: 0.1115 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "11/11 - 0s - loss: 0.1095 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "11/11 - 0s - loss: 0.1077 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "11/11 - 0s - loss: 0.1058 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "11/11 - 0s - loss: 0.1040 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "11/11 - 0s - loss: 0.1023 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "11/11 - 0s - loss: 0.1005 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "11/11 - 0s - loss: 0.0989 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "11/11 - 0s - loss: 0.0972 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "11/11 - 0s - loss: 0.0956 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "11/11 - 0s - loss: 0.0941 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d6855b2c08>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile( loss = 'categorical_crossentropy', optimizer = 'adam',\n",
    "               metrics = [ 'accuracy' ] )\n",
    "model.fit( X, y, epochs = 200, verbose = 2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인수 : model - 모델, t - 토크나이거, \n",
    "#        current_word - 현재 단어, n - 예측을 반복할 횟수\n",
    "def sentence_generation( model, t, current_word, n ):\n",
    "    init_word = current_word\n",
    "    sentence = ''\n",
    "    \n",
    "    for _ in range( n ):\n",
    "        encoded = t.texts_to_sequences( [ current_word ] )[ 0 ]\n",
    "        encoded = pad_sequences( [ encoded ], maxlen = 5, \n",
    "                                padding = 'pre' )\n",
    "        result = model.predict_classes( encoded, verbose = 0 ) \n",
    "        # 입력한 X(현재단어)에 대한 y(예측한 단어)를 result에 저장\n",
    "        \n",
    "        for word, index in t.word_index.items():\n",
    "            if index == result: # 예측할 단어와 동일한 단어가 있다면\n",
    "                break;\n",
    "        current_word = current_word + ' ' + word\n",
    "        # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\n",
    "    sentence = init_word + sentence\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "경마장에 있는 말이 뛰고 있다\n"
     ]
    }
   ],
   "source": [
    "print( sentence_generation( model, t, '경마장에', 4 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "그의 말이 법이다\n"
     ]
    }
   ],
   "source": [
    "print( sentence_generation( model, t, '그의', 2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "가는 말이 고와야 오는 말이 곱다\n"
     ]
    }
   ],
   "source": [
    "print( sentence_generation( model, t, '가는', 5 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
